% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bayes_adjusted.R
\name{bayes_adjusted}
\alias{bayes_adjusted}
\title{Bayesian adjusted rating (beta-binomial shrinkage) for arbitrary scale}
\usage{
bayes_adjusted(
  rating,
  n,
  min_score = 1,
  max_score = 5,
  prior_mean = (min_score + max_score)/2,
  prior_n = 20
)
}
\arguments{
\item{rating}{Numeric vector of observed ratings}

\item{n}{Numeric vector of number of reviews (same length as \code{rating})}

\item{min_score}{Minimum rating (default = 1)}

\item{max_score}{Maximum rating (default = 5)}

\item{prior_mean}{Prior expected rating on the same scale as \code{rating}.
Represents the rating you would trust if no reviews were available.
Default is the midpoint of the scale.}

\item{prior_n}{Prior strength (pseudo-count). Interpreted as the
\emph{minimum trusted number of reviews}. Ratings based on fewer
reviews than \code{prior_n} are strongly shrunk; ratings with many reviews
are minimally affected.}
}
\value{
Numeric vector of Bayesian-adjusted ratings on the original scale
}
\description{
Computes a Bayesian shrinkage estimate of a rating, adjusting for
the number of reviews and a prior belief. This is useful because
small numbers of reviews can produce extreme or unreliable average ratings.
}
\section{Why Bayesian adjusted scores?}{

Raw averages are extremely sensitive when review counts are small.
A feature with a single 5-star rating should not outrank another with
hundreds of reviews averaging 4.8. \cr
\code{bayes_adjusted()} solves this by shrinking each observed rating
toward a prior value (\code{prior_mean}), with the strength of the pull
controlled by \code{prior_n}. \cr
High-volume items are barely affected; low-volume items are pulled
toward a neutral rating until more evidence is available.
}

\section{Formula}{

\strong{Under the hood: Beta–Binomial shrinkage}

First, each rating is mapped to the 0-1 scale:
\deqn{ p = \frac{rating - min\_score}{max\_score - min\_score} }

With \eqn{n} reviews, the data contribute \eqn{r = p \cdot n} "successes".

The prior is expressed as a Beta distribution with parameters:
\deqn{ \alpha = prior\_mean \cdot prior\_n }
\deqn{ \beta  = (1 - prior\_mean) \cdot prior\_n }

Combining the prior and observed data yields a posterior mean:
\deqn{
  \hat{\theta}
  = \frac{\alpha + r}{\alpha + \beta + n}
}

Intuition:
\itemize{
  \item If \eqn{n} is small, the prior dominates and \eqn{\hat{\theta}} stays close to \code{prior_mean}
  \item As \eqn{n} grows large, \eqn{\hat{\theta}} approaches the observed average
}

Finally, \eqn{\hat{\theta}} is mapped back to the original rating scale.
}

\examples{
# Example 1: Standard 1–5 rating scale
ratings <- c(5, 2, 4.5, 5)
n_reviews <- c(1, 5, 30, 500)

# Raw averages vs Bayesian shrinkage
data.frame(
  rating = ratings,
  n = n_reviews,
  bayes = bayes_adjusted(ratings, n_reviews)
)

# The item with 1 review (5 stars) is pulled downward strongly,
# while the item with 500 reviews remains almost unchanged.

# Example 2: Custom 0–10 scale with a neutral prior (mean = 5) and stronger shrinkage
ratings2 <- c(10, 7, 2)
n_reviews2 <- c(2, 20, 500)

bayes_adjusted(ratings2, n_reviews2,
               min_score = 0, max_score = 10,
               prior_mean = 5, prior_n = 30)

# Items with very few reviews are pulled strongly toward 5 (neutral),
# while the item with 500 reviews remains close to its observed average.

}
\seealso{
\code{\link{wilson_score}} for conservative, uncertainty-aware ranking of ratings
based on the Wilson confidence interval.
}
